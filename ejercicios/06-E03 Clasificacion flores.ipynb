{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de flores\n",
    "\n",
    "El código mostrado en este ejercicio utiliza las fotos de flores de la carpeta `datos/flower_photos` para entrenar un modelo de clasificación. Las fotos están divididas en subcarpetas con el nombre del tipo de flor (roses, tulips, daisy,...).\n",
    "\n",
    "Las fotos se cargan en dos conjuntos de datos (datasets) de TensorFlow, uno para el entrenamiento y otro para la validación.\n",
    "\n",
    "**Objetivos:**\n",
    "\n",
    "- Conocer algunas de las funciones de TensorFlow para la carga de datos y el preprocesamiento.\n",
    "- Seguir trabajando con la API de Keras para crear modelos de redes neuronales.\n",
    "\n",
    "### Tareas del estudiante\n",
    "1. Activar el entorno **curso-python-ml-rn** y ejecutar el código de todas las celdas.\n",
    "2. Revisar cómo se cargan las fotos en los datasets.\n",
    "   1. ¿Qué función se utiliza?\n",
    "   2. ¿En qué propiedad del dataset se guardan los nombres de las carpetas (clases)?\n",
    "   3. ¿Qué porcentaje de las fotos se usa para el entrenamiento?\n",
    "3. Escalado\n",
    "   1. ¿Cómo se llama la función que se utiliza para hacer el escalado?\n",
    "   2. ¿En qué otra parte del código está esta función? Aunque está comentada.\n",
    "4. Creación del modelo\n",
    "   1. ¿Cuántas capas ocultas tiene la red neuronal?\n",
    "   2. ¿Cuántas salidas tiene la capa de salida?\n",
    "   3. Busque en la documentación de TensorFlow qué características tiene cada capa.\n",
    "5. Compilación\n",
    "   1. Busque en la documentación de TensorFlow qué significan cada uno de los argumentos que se han pasado a la función `compile`.\n",
    "6. Entrenamiento\n",
    "   1. ¿Qué significa el epoch?\n",
    "   2. Prueba a cambiar el epoch y comprueba si se obtienen mejores resultados en la evaluación.\n",
    "   3. Hay una función que muestra un resumen del modelo en formato texto. Busque en la documentación de TensorFlow cuál es la función y utilícela en la **celda vacía que está después de entrenar el modelo**.\n",
    "7. Evaluación del modelo\n",
    "   1. ¿Crees que son buenos los resultados de la evaluación?\n",
    "   2. ¿Cómo se llama lo que está sucediendo?\n",
    "8. Mejora del modelo\n",
    "   1. **Aumento de datos**: Esta es una técnica para generar más datos de entrenamiento a partir de modificar los que ya existen.\n",
    "      1. Revise el código que está en las últimas celdas, donde se crea una secuencia con el nombre `data_augmentation`.\n",
    "      2. Utilice dicha secuencia como el primer paso al crear el modelo de clasificación.\n",
    "   2. **Dropout**: Es una técnica de regularización de la red neuronal que consiste en que durante el entrenamiento se descarta de manera aleatoria un porcentaje de los datos de salida de una capa.\n",
    "      1. Al crear el modelo, introduzca una nueva capa `Dropout` justo antes de la capa Flatten y que descarte el 20% de los datos.\n",
    "9. Vuelva a evaluar el modelo luego de hacer los dos cambios anteriores.\n",
    "10. Predicción con una nueva foto\n",
    "    1. Busque en internet una foto de una flor y vaya a la última celda de este notebook y copie en URL y ejecute la celda, para predecir la clase de la foto.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de flores\n",
    "\n",
    "El código mostrado en este ejercicio utiliza las fotos de flores de la carpeta `datos/flower_photos´ para entrenar un modelo de clasificácion. Las fotos están divididas en subcarpetas con el nombre del tipo de flor (rose,tulip,daisy,...).\n",
    "\n",
    "Las fotos se cargan hacia dos datasets de TensorFlow, uno para el entrenamiento y el otro para la validación. \n",
    "\n",
    "**Objetivos:**\n",
    "\n",
    "- Conocer algunas de las funciones de TensorFlow para la carga de datos y el preprocesamiento\n",
    "- Seguir trabajando con la API de Keras para crear modelos de redes neuronales \n",
    "\n",
    "### Tareas del estudiante\n",
    "1. Activar el entorno **curso-python-ml-rn** y ejecutar el código de todas las celdas\n",
    "2. Revisar cómo se cargan las fotos en los datasets. \n",
    "   1. ¿Qué función se utiliza?\n",
    "   2. ¿En cuál propiedad del dataset se guardan los nombres de las carpetas (clases)? \n",
    "   3. ¿Qué porcentaje de las fotos se usa para el entrenamiento?\n",
    "3. Escalado\n",
    "   1. ¿Cómo se llama la función que se utiliza para hacer el escalado?\n",
    "   2. ¿En que otra parte del código está esta función? Aunque está comentada.\n",
    "4. Creación del modelo\n",
    "   1. ¿Cuántas capas ocultas tiene la red neuronal?\n",
    "   2. ¿Cuántas salidas tiene la capa de salida?\n",
    "   3. Busque en la documentación de TensorFlow que carácterística tiene cada capa.\n",
    "5. Compilación\n",
    "   1. Busque en la documentación de TensorFlow que significan cada uno de los argumentos que se han pasado a la función `compile`.\n",
    "6. Entrenamiento\n",
    "   1. ¿Qué significa el Epoch?\n",
    "   2. Prueba a cambiar el Epoch y comprueba si se obtienen mejores resultados en la evaluación.\n",
    "   3. Hay una función que muestra un resúmen del modelo en formato texto. Busqua en la documentación de TensorFlow cuál es la función y utilícela en la **celda vacía que está después de que se entrena el modelo**.\n",
    "7. Evaluación del modelo\n",
    "   1. ¿Crees que son buenos los resultados de la evaluación?\n",
    "   2. ¿Cómo se llama lo que stá sucediendo?\n",
    "8. Mejora del modelo\n",
    "   1. **Aumento de datos**: Esta es una técnica para generar más datos de entrenamiento a partir de modificar los que ya existen.\n",
    "      1. Revise el código que está en las últimas celdas, donde se crea una secuencia con el nombre `data_augmentation`.\n",
    "      2. Utilice dicha secuencia como el primer paso del modelo de clasificación.\n",
    "   2. **Dropout**: Es una técnica de regularización de la red neuronal que consiste en que durante el entrenamiento se descartan de manera aleatoria un porcentaje de los datos de salida de una capa. \n",
    "      1. Al crear el modelo, introduzca una nueva capa ´layer.Dropout´ justo antes de la capa Flattern y que descarte el 20% de los datos.\n",
    "9. Vuelva a evaluar el modelo luego de hacer los dos cambios anteriores.\n",
    "10. Predicción con una nueva foto\n",
    "    1.  Busque en internet una foto de una flor y vaya a la útima celda de este notebook y copie la URL para predecir la clase de la foto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta variable de entorno es necesaria para evitar que un error fatal de TensorFlow \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Ignorando los warnings de TensorFlow\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta a la carpeta con las imágenes\n",
    "IMAGES_DIR_PATH =  'datos/flower_photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contando cuantas imágenes hay \n",
    "data_dir = pathlib.Path(IMAGES_DIR_PATH)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando una rosa\n",
    "roses = list(data_dir.glob('roses/*'))\n",
    "PIL.Image.open(str(roses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 180\n",
    "IMG_WIDTH = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el dataset de entrenamiento\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "  batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el dataset de validación\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "  batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardando los nombres de las clases en una variable\n",
    "class_names = train_ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando algunas imágenes\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando el dataset para un mejor rendimiento\n",
    "# - Cache de las imágenes\n",
    "# - Solapando el preprocesamiento y la ejcución al entrenar el modelo\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalando los datos para que los valores de cada pixel estén entre 0 y 1. Ahora están entre 0 y 255.\n",
    "\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo con la API secuencial de Keras\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  # layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilando el modelo\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenando el modelo\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilice esta celda para llamar a una función que muestre \n",
    "# un resumen de la estructura del modelo. Busque el ombre de la función en la documentatción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando los resultados del entrenamiento\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aumento de datos (Data augmentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiendo una secuencia para generar nuevas fotos a patrtir de transformar las que existen\n",
    "\n",
    "data_augmentation = Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(IMG_HEIGHT,\n",
    "                                  IMG_WIDTH,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haciendo una prueba con la secuencia creada arriba\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicción con una foto nueva**\n",
    "\n",
    "Busque alguna foto de una flor internet, que sea de alguna de las clases de flores que entrenamos, y copie el URL en la celda de abajo y ejecútela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWER_URL = \"\"  #Copie aquí el URL de una foto \n",
    "if FLOWER_URL != \"\":\n",
    "\n",
    "    flower_path = tf.keras.utils.get_file('Red_sunflower', origin=FLOWER_URL)\n",
    "\n",
    "    img = tf.keras.utils.load_img(\n",
    "        flower_path, target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"Esta imagen parece ser de una {} con una seguridad del {:.2f} %.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curso-python-ml-rn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
